{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Before usage of this API, \n",
    "please ensure the following packages are installed. \n",
    "\n",
    "Tensorflow: 1.11.0\n",
    "Keras: 2.2.4\n",
    "NumPy: 1.15.1\n",
    "\n",
    "Note that you can directly install these packages in ipython notebook\n",
    "through commands like \"!pip install tensorflow==1.11\"\n",
    "'''\n",
    "\n",
    "# Let's start our demestration\n",
    "# For this grid, we import some packages and utils.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras \n",
    "\n",
    "import random, math\n",
    "\n",
    "# You can use the API without creating an utils instance, \n",
    "# We create an utils instance here for printing some information \n",
    "# to illustrate that our operators function correctly \n",
    "import utils\n",
    "utils = utils.GeneralUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_datas shape: (5000, 784)\n",
      "train_labels shape: (5000,)\n",
      "test_datas shape: (1000, 784)\n",
      "test_labels shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare training dataset and untrained model\n",
    "# Users can their our own dataset and model\n",
    "import network \n",
    "network = network.SimplyNetwork()\n",
    "\n",
    "# model is a simple FC(fully-connected) neural network\n",
    "# dataset is a subset from MNIST dataset with 5000 training data and 1000 testing data\n",
    "model = network.create_debug_model()\n",
    "(train_datas, train_labels), (test_datas, test_labels) = network.load_data()\n",
    "\n",
    "print('train_datas shape:', train_datas.shape)\n",
    "print('train_labels shape:', train_labels.shape)\n",
    "print('test_datas shape:', test_datas.shape)\n",
    "print('test_labels shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 105us/step\n",
      "model accurancy: 92.60%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile and train our example model\n",
    "model = network.compile_model(model)\n",
    "model = network.train_model(model, train_datas, train_labels)\n",
    "\n",
    "# Let's print the accurancy of our example model to see whether it's trained \n",
    "# You should see accurancy around 92%\n",
    "network.evaluate_model(model, test_datas, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of source-level mutation operators API\n",
    "import model_mut_operators\n",
    "model_mut_opts = model_mut_operators.ModelMutationOperators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GF (Gaussian Fuzzing), see https://github.com/KuoTzu-yang/DeepMutation for more explanation\n",
    "# Notice you should re-compile the mutated model \n",
    "mutation_ratio = 0.1\n",
    "STD=0.2\n",
    "GF_model = model_mut_opts.GF_mut(model, mutation_ratio, STD=STD)\n",
    "GF_model = network.compile_model(GF_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before GF\n",
      "1000/1000 [==============================] - 0s 139us/step\n",
      "model accurancy: 92.60%\n",
      "\n",
      "After GF, where the mutation ratio is 0.1 and STD is 0.2\n",
      "1000/1000 [==============================] - 0s 133us/step\n",
      "GF mutation operator executed\n",
      "Mutated model, accurancy: 82.40%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Let's evaluate the performance of the mutated model compared to the original model. \n",
    "Either increase of mutation_ratio or STD will give rise to a decrease in accuracy, which is consistent with our expectation.  \n",
    "You can turn the value of mutation_ratio and STD to observe the difference of accuracy via the print function. \n",
    "'''\n",
    "utils.print_messages_MMM_generators('GF', network=network, test_datas=test_datas, test_labels=test_labels, model=model, mutated_model=GF_model, STD=STD, mutation_ratio=mutation_ratio) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "WS (Weight Shuffling), see https://github.com/KuoTzu-yang/DeepMutation for more explanation\n",
    "Users should re-compile the mutated model. \n",
    "For WS, we simply demostrate the usage of WS mutation operator. \n",
    "'''\n",
    "mutation_ratio = 0.1\n",
    "WS_model = model_mut_opts.WS_mut(model, mutation_ratio)\n",
    "WS_model = network.compile_model(WS_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NEB (Neuron Effect Block), see https://github.com/KuoTzu-yang/DeepMutation for more explanation\n",
    "Users should re-compile the mutated model. \n",
    "For NEB, we simply demostrate the usage of NEB mutation operator. \n",
    "'''\n",
    "mutation_ratio = 0.1\n",
    "NEB_model = model_mut_opts.NEB_mut(model, mutation_ratio)\n",
    "NEB_model = network.compile_model(NEB_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NAI (Neuron Activation Inverse), see https://github.com/KuoTzu-yang/DeepMutation for more explanation\n",
    "Users should re-compile the mutated model. \n",
    "For NAI, we simply demostrate the usage of NAI mutation operator. \n",
    "'''\n",
    "mutation_ratio = 0.1\n",
    "NAI_model = model_mut_opts.NAI_mut(model, mutation_ratio)\n",
    "NAI_model = network.compile_model(NAI_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NS (Neuron Switch), see https://github.com/KuoTzu-yang/DeepMutation for more explanation\n",
    "Users should re-compile the mutated model. \n",
    "For NS, we simply demostrate the usage of NS mutation operator. \n",
    "'''\n",
    "mutation_ratio = 0.1\n",
    "NS_model = model_mut_opts.NS_mut(model, mutation_ratio)\n",
    "NS_model = network.compile_model(NS_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before LD\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 51,994\n",
      "Trainable params: 51,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1000/1000 [==============================] - 0s 252us/step\n",
      "model accurancy: 92.60%\n",
      "\n",
      "After LD\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1_copy_LD (Dense)      (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_1_copy_LD (Dropout)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2_copy_LD (Dense)      (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_3_copy_LD (Dense)      (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5_copy_LD (Dense)      (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 51,722\n",
      "Trainable params: 51,722\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1000/1000 [==============================] - 0s 117us/step\n",
      "LD mutation operator executed\n",
      "Mutated model, accurancy:  8.10%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "LD (Layer Deactivation), see https://github.com/KuoTzu-yang/DeepMutation for more explanation\n",
    "Users should re-compile the mutated model. \n",
    "'''\n",
    "LD_model = model_mut_opts.LD_mut(model)\n",
    "LD_model = network.compile_model(LD_model)\n",
    "utils.print_messages_MMM_generators('LD', network=network, test_datas=test_datas, test_labels=test_labels, model=model, mutated_model=LD_model) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before LAm\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 51,994\n",
      "Trainable params: 51,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1000/1000 [==============================] - 0s 202us/step\n",
      "model accurancy: 92.60%\n",
      "\n",
      "After LAm\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1_copy_LAm (Dense)     (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_1_copy_LAm (Dropout) (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2_copy_LAm (Dense)     (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_3_copy_LAm (Dense)     (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_3_copy_insert (Dense)  (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_4_copy_LAm (Dense)     (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5_copy_LAm (Dense)     (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 52,266\n",
      "Trainable params: 52,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1000/1000 [==============================] - 0s 132us/step\n",
      "LAm mutation operator executed\n",
      "Mutated model, accurancy: 11.80%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "LAm (Layer Addition for model-level), see https://github.com/KuoTzu-yang/DeepMutation for more explanation\n",
    "Users should re-compile the mutated model. \n",
    "'''\n",
    "LAm_model = model_mut_opts.LAm_mut(model)\n",
    "LAm_model = network.compile_model(LAm_model)\n",
    "utils.print_messages_MMM_generators('LAm', network=network, test_datas=test_datas, test_labels=test_labels, model=model, mutated_model=LAm_model) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before AFRm\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 51,994\n",
      "Trainable params: 51,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1000/1000 [==============================] - 0s 401us/step\n",
      "model accurancy: 92.60%\n",
      "\n",
      "After AFRm\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1_copy_AFRm (Dense)    (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_1_copy_AFRm (Dropout (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2_copy_AFRm (Dense)    (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_3_copy_AFRm (Dense)    (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_4_copy_AFRm (Dense)    (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5_copy_AFRm (Dense)    (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 51,994\n",
      "Trainable params: 51,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1000/1000 [==============================] - 0s 236us/step\n",
      "AFRm mutation operator executed\n",
      "Mutated model, accurancy: 54.90%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "AFRm (Activation Function Removal for model-level), see https://github.com/KuoTzu-yang/DeepMutation for more explanation\n",
    "Users should re-compile the mutated model. \n",
    "'''\n",
    "AFRm_model = model_mut_opts.AFRm_mut(model)\n",
    "AFRm_model = network.compile_model(AFRm_model)\n",
    "utils.print_messages_MMM_generators('AFRm', network=network, test_datas=test_datas, test_labels=test_labels, model=model, mutated_model=AFRm_model) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
